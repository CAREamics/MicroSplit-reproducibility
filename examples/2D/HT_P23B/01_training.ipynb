{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code for dataset HT_P23B 2D\n",
    "\n",
    "Training is done in a supervised way. For every input patch, we have the two corresponding target patches using which we train our VSE (Variational Splitting Encoder decoder Network) with KL-divergence loss and a per-channel likelihood loss, following denoiSplit[ref]. In the likelihood computation, Noise models are used. Besides the primary input patch, we also feed LC inputs, originally introduced in uSplit[ref], to the network to make available information about larger spatial context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important ! \n",
    "\n",
    "This step can be skipped! Only run this notebook if you want to train the microsplit model from scratch or finetune. Pretrained model checkpoint is available \n",
    "By default this notebook runs example training for 5 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pooch\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from careamics.lightning import VAEModule\n",
    "\n",
    "from microsplit_reproducibility.configs.factory import (\n",
    "    create_algorithm_config,\n",
    "    get_likelihood_config,\n",
    "    get_loss_config,\n",
    "    get_model_config,\n",
    "    get_optimizer_config,\n",
    "    get_training_config,\n",
    "    get_lr_scheduler_config,\n",
    ")\n",
    "from microsplit_reproducibility.utils.callbacks import get_callbacks\n",
    "from microsplit_reproducibility.utils.io import load_checkpoint, load_checkpoint_path\n",
    "from microsplit_reproducibility.datasets import create_train_val_datasets\n",
    "from microsplit_reproducibility.utils.utils import (\n",
    "    plot_training_metrics,\n",
    "    plot_input_patches,\n",
    "    plot_training_outputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.configs.parameters.HT_P23B_2D import (\n",
    "    get_microsplit_parameters,\n",
    ")\n",
    "from microsplit_reproducibility.configs.data.HT_P23B_2D import get_data_configs\n",
    "from microsplit_reproducibility.datasets.HT_P23B import get_train_val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pooch.create(\n",
    "    path=\"./data\",\n",
    "    base_url=\"https://download.fht.org/jug/microsplit/ht_p23b\",\n",
    "    registry={\"ht_p23b.zip\": None},\n",
    ")\n",
    "\n",
    "NOISE_MODELS = pooch.create(\n",
    "    path=\"./noise_models\",\n",
    "    base_url=\"https://download.fht.org/jug/microsplit/ht_p23b/2d\",\n",
    "    registry={\n",
    "        \"nm_ht_p23b_2d_ch1.npz\": None,\n",
    "        \"nm_ht_p23b_2d_ch2.npz\": None,\n",
    "    },\n",
    ")\n",
    "\n",
    "MODEL_CHECKPOINTS = pooch.create(\n",
    "    path=\"./checkpoints\",\n",
    "    base_url=\"https://download.fht.org/jug/microsplit/ht_p23b_2d\",\n",
    "    registry={\"best.ckpt\": None, \"last.ckpt\": None},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'best.ckpt' from 'https://download.fht.org/jug/microsplit/ht_p23b_2d/best.ckpt' to '/home/igor.zubarev/projects/microSplit-reproducibility/examples/2D/HT_P23B/checkpoints'.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://download.fht.org/jug/microsplit/ht_p23b_2d/best.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m DATA\u001b[38;5;241m.\u001b[39mfetch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mht_p23b.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, processor\u001b[38;5;241m=\u001b[39mpooch\u001b[38;5;241m.\u001b[39mUnzip())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m MODEL_CHECKPOINTS\u001b[38;5;241m.\u001b[39mregistry:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mMODEL_CHECKPOINTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mf\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/mamba/envs/splits/lib/python3.9/site-packages/pooch/core.py:589\u001b[0m, in \u001b[0;36mPooch.fetch\u001b[0;34m(self, fname, processor, downloader, progressbar)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m downloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m         downloader \u001b[38;5;241m=\u001b[39m choose_downloader(url, progressbar\u001b[38;5;241m=\u001b[39mprogressbar)\n\u001b[0;32m--> 589\u001b[0m     \u001b[43mstream_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mknown_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpooch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_if_failed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_if_failed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processor(\u001b[38;5;28mstr\u001b[39m(full_path), action, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/localscratch/mamba/envs/splits/lib/python3.9/site-packages/pooch/core.py:807\u001b[0m, in \u001b[0;36mstream_download\u001b[0;34m(url, fname, known_hash, downloader, pooch, retry_if_failed)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# Stream the file to a temporary so that we can safely check its\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;66;03m# hash before overwriting the original.\u001b[39;00m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m temporary_file(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(fname\u001b[38;5;241m.\u001b[39mparent)) \u001b[38;5;28;01mas\u001b[39;00m tmp:\n\u001b[0;32m--> 807\u001b[0m         \u001b[43mdownloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m         hash_matches(tmp, known_hash, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(fname\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m    809\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mmove(tmp, \u001b[38;5;28mstr\u001b[39m(fname))\n",
      "File \u001b[0;32m/localscratch/mamba/envs/splits/lib/python3.9/site-packages/pooch/downloaders.py:221\u001b[0m, in \u001b[0;36mHTTPDownloader.__call__\u001b[0;34m(self, url, output_file, pooch, check_only)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 221\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size)\n\u001b[1;32m    223\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/localscratch/mamba/envs/splits/lib/python3.9/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://download.fht.org/jug/microsplit/ht_p23b_2d/best.ckpt"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(NOISE_MODELS.registry):\n",
    "    NOISE_MODELS.fetch(f\"nm_ht_p23b_2d_ch{i+1}.npz\")\n",
    "\n",
    "DATA.fetch(\"ht_p23b.zip\", processor=pooch.Unzip())\n",
    "\n",
    "for f in MODEL_CHECKPOINTS.registry:\n",
    "    MODEL_CHECKPOINTS.fetch(f\"{f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading nm_ht_p23b_2d_channel_1.npz\n",
      "Downloading nm_ht_p23b_2d_channel_2.npz\n"
     ]
    }
   ],
   "source": [
    "train_data_config, val_data_config, test_data_configs = get_data_configs()\n",
    "experiment_params = get_microsplit_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data_type', <DataType.ExpMicroscopyV2: 6>)\n",
      "('depth3D', 1)\n",
      "('datasplit_type', <DataSplitType.Train: 1>)\n",
      "('num_channels', 2)\n",
      "('ch1_fname', None)\n",
      "('ch2_fname', None)\n",
      "('ch_input_fname', None)\n",
      "('input_is_sum', False)\n",
      "('input_idx', None)\n",
      "('target_idx_list', None)\n",
      "('start_alpha', None)\n",
      "('end_alpha', None)\n",
      "('image_size', (64, 64))\n",
      "('grid_size', 32)\n",
      "('empty_patch_replacement_enabled', False)\n",
      "('empty_patch_replacement_channel_idx', None)\n",
      "('empty_patch_replacement_probab', None)\n",
      "('empty_patch_max_val_threshold', None)\n",
      "('uncorrelated_channels', False)\n",
      "('uncorrelated_channel_probab', 0.5)\n",
      "('poisson_noise_factor', -1.0)\n",
      "('synthetic_gaussian_scale', 228.0)\n",
      "('input_has_dependant_noise', True)\n",
      "('enable_gaussian_noise', False)\n",
      "('allow_generation', False)\n",
      "('training_validtarget_fraction', None)\n",
      "('deterministic_grid', None)\n",
      "('enable_rotation_aug', False)\n",
      "('max_val', None)\n",
      "('overlapping_padding_kwargs', {'mode': 'reflect'})\n",
      "('print_vars', False)\n",
      "('normalized_input', True)\n",
      "('use_one_mu_std', True)\n",
      "('train_aug_rotate', True)\n",
      "('enable_random_cropping', True)\n",
      "('multiscale_lowres_count', 1)\n",
      "('tiling_mode', <TilingMode.ShiftBoundary: 2>)\n",
      "('target_separate_normalization', True)\n",
      "('mode_3D', False)\n",
      "('trainig_datausage_fraction', 1.0)\n",
      "('validtarget_random_fraction', None)\n",
      "('validation_datausage_fraction', 1.0)\n",
      "('random_flip_z_3D', False)\n",
      "('padding_kwargs', {'mode': 'reflect'})\n",
      "('subdset_type', <SubDsetType.MultiChannel: 2>)\n"
     ]
    }
   ],
   "source": [
    "for k in train_data_config:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'denoisplit',\n",
       " 'loss_type': 'denoisplit_musplit',\n",
       " 'img_size': (64, 64),\n",
       " 'target_channels': 2,\n",
       " 'multiscale_count': 1,\n",
       " 'predict_logvar': 'pixelwise',\n",
       " 'nm_paths': ['noise_models/nm_ht_p23b_2d_channel_1.npz',\n",
       "  'noise_models/nm_ht_p23b_2d_channel_2.npz'],\n",
       " 'kl_type': 'kl_restricted',\n",
       " 'batch_size': 32,\n",
       " 'lr': 0.001,\n",
       " 'lr_scheduler_patience': 30,\n",
       " 'earlystop_patience': 200,\n",
       " 'num_epochs': 400,\n",
       " 'num_workers': 0,\n",
       " 'mmse_count': 10,\n",
       " 'grid_size': 32}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding is not used with this alignement style\n",
      "\n",
      "Padding is not used with this alignement style\n",
      "MultiFileDset avg height: 1584, avg width: 1584, count: 58\n",
      "\n",
      "Padding is not used with this alignement style\n",
      "MultiFileDset avg height: 1584, avg width: 1584, count: 12\n",
      "\n",
      "Padding is not used with this alignement style\n",
      "MultiFileDset avg height: 1584, avg width: 1584, count: 12\n"
     ]
    }
   ],
   "source": [
    "train_dset, val_dset, _, data_stats = create_train_val_datasets(\n",
    "    datapath=DATA.path / \"ht_p23b.zip.unzip/datafiles\",\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=get_train_val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get experiment configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params[\"data_stats\"] = data_stats  # TODO rethink\n",
    "\n",
    "loss_config = get_loss_config(**experiment_params)\n",
    "model_config = get_model_config(**experiment_params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **experiment_params\n",
    ")\n",
    "training_config = get_training_config(**experiment_params)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**experiment_params)\n",
    "optimizer_config = get_optimizer_config(**experiment_params)\n",
    "\n",
    "experiment_config = create_algorithm_config(\n",
    "    algorithm=experiment_params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModule(algorithm_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint (optional)\n",
    "\n",
    "It's possible to load a checkpoint to continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = load_checkpoint_path(\"checkpoints\", best=True)\n",
    "model = VAEModule.load_from_checkpoint(\n",
    "    ckpt_path, algorithm_config=experiment_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_patches(dataset=train_dset, num_channels=3, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Only 5 epochs for the sake of the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=training_config.num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=get_callbacks(\"/checkpoints\"),\n",
    "    precision=training_config.precision,\n",
    "    gradient_clip_val=training_config.gradient_clip_val,\n",
    "    gradient_clip_algorithm=training_config.gradient_clip_algorithm,\n",
    ")\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dloader,\n",
    "    val_dataloaders=val_dloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metrics(list(Path(\"csv_logs\").rglob(\"metrics.csv\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_outputs(val_dset, trainer.model, num_channels=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
