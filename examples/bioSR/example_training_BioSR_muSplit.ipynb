{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import socket\n",
    "from copy import deepcopy\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional, Union, Callable\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "from pytorch_lightning import Trainer\n",
    "from numpy.typing import NDArray\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from careamics.config import VAEAlgorithmConfig\n",
    "from careamics.config.architectures import LVAEModel\n",
    "from careamics.config.likelihood_model import (\n",
    "    GaussianLikelihoodConfig,\n",
    "    NMLikelihoodConfig,\n",
    ")\n",
    "from careamics.config.nm_model import GaussianMixtureNMConfig, MultiChannelNMConfig\n",
    "from careamics.config.optimizer_models import LrSchedulerModel, OptimizerModel\n",
    "from careamics.lightning import VAEModule\n",
    "from careamics.models.lvae.noise_models import noise_model_factory\n",
    "\n",
    "from careamics.lvae_training.dataset import LCMultiChDloader, MultiChDloader\n",
    "from careamics.lvae_training.dataset import DatasetConfig, DataSplitType, DataType\n",
    "\n",
    "from examples.bioSR.load_data_mrc import get_train_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some parameters for the current training simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size: int = 64\n",
    "\"\"\"Spatial size of the input image.\"\"\"\n",
    "target_channels: int = 2\n",
    "\"\"\"Number of channels in the target image.\"\"\"\n",
    "multiscale_count: int = 3\n",
    "\"\"\"The number of LC inputs plus one (the actual input).\"\"\"\n",
    "predict_logvar: Optional[Literal[\"pixelwise\"]] = \"pixelwise\"\n",
    "\"\"\"Whether to compute also the log-variance as LVAE output.\"\"\"\n",
    "loss_type: Optional[Literal[\"musplit\", \"denoisplit\", \"denoisplit_musplit\"]] = \"musplit\"\n",
    "\"\"\"The type of reconstruction loss (i.e., likelihood) to use.\"\"\"\n",
    "nm_paths: Optional[tuple[str]] = [\n",
    "    \"/group/jug/ashesh/training_pre_eccv/noise_model/2402/221/GMMNoiseModel_ER-GT_all.mrc__6_4_Clip0.0-1.0_Sig0.125_UpNone_Norm0_bootstrap.npz\",\n",
    "    \"/group/jug/ashesh/training_pre_eccv/noise_model/2402/225/GMMNoiseModel_Microtubules-GT_all.mrc__6_4_Clip0.0-1.0_Sig0.125_UpNone_Norm0_bootstrap.npz\"\n",
    "]\n",
    "\"\"\"The paths to the pre-trained noise models for the different channels.\"\"\"\n",
    "# TODO: add denoisplit-musplit weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace this with `careamics,config.training_model.TrainingConfig` once updated\n",
    "class TrainingConfig(BaseModel):\n",
    "    \"\"\"Configuration for training a VAE model.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        validate_assignment=True, arbitrary_types_allowed=True, extra=\"allow\"\n",
    "    )\n",
    "\n",
    "    batch_size: int = 32\n",
    "    \"\"\"The batch size for training.\"\"\"\n",
    "    precision: int = 16\n",
    "    \"\"\"The precision to use for training.\"\"\"\n",
    "    lr: float = 1e-3\n",
    "    \"\"\"The learning rate for training.\"\"\"\n",
    "    lr_scheduler_patience: int = 30\n",
    "    \"\"\"The patience for the learning rate scheduler.\"\"\"\n",
    "    earlystop_patience: int = 200\n",
    "    \"\"\"The patience for the learning rate scheduler.\"\"\"\n",
    "    max_epochs: int = 50\n",
    "    \"\"\"The maximum number of epochs to train for.\"\"\"\n",
    "    num_workers: int = 4\n",
    "    \"\"\"The number of workers to use for data loading.\"\"\"\n",
    "    grad_clip_norm_value: int = 0.5\n",
    "    \"\"\"The value to use for gradient clipping (see lightning `Trainer`).\"\"\"\n",
    "    gradient_clip_algorithm: int = 'value'\n",
    "    \"\"\"The algorithm to use for gradient clipping (see lightning `Trainer`).\"\"\"\n",
    "\n",
    "train_config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create `Dataset` and `Dataloader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters for BIOSR dataset\n",
    "def get_data_configs() -> tuple[DatasetConfig, DatasetConfig]:\n",
    "    padding_kwargs = {\"mode\": \"reflect\"}\n",
    "    train_data_config = DatasetConfig(\n",
    "        datasplit_type=DataSplitType.Train,\n",
    "        image_size=img_size,\n",
    "        num_channels=target_channels,\n",
    "        multiscale_lowres_count=multiscale_count,\n",
    "        data_type=DataType.BioSR_MRC,\n",
    "        ch1_fname=\"ER/GT_all.mrc\",\n",
    "        ch2_fname=\"CCPs/GT_all.mrc\",\n",
    "        enable_gaussian_noise=True,\n",
    "        synthetic_gaussian_scale=5100,\n",
    "        input_has_dependant_noise=True,\n",
    "        enable_random_cropping=True,\n",
    "        allow_generation=True,\n",
    "        padding_kwargs=padding_kwargs,\n",
    "        overlapping_padding_kwargs=padding_kwargs\n",
    "    )\n",
    "    \n",
    "    val_data_config = train_data_config.model_copy(\n",
    "        update=dict(\n",
    "            datasplit_type=DataSplitType.Val,\n",
    "            allow_generation=False,  # No generation during validation\n",
    "            enable_random_cropping=False,  # No random cropping on validation.\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return train_data_config, val_data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    datapath: str,\n",
    "    train_config: DatasetConfig,\n",
    "    val_config: DatasetConfig,\n",
    "    load_data_func: Callable[..., NDArray]\n",
    ") -> tuple[Dataset, Dataset, tuple[float, float]]:\n",
    "    if train_config.multiscale_lowres_count > 1:\n",
    "        dataset_class = LCMultiChDloader\n",
    "    else: \n",
    "        dataset_class = MultiChDloader\n",
    "        \n",
    "    train_data = dataset_class(\n",
    "        train_config,\n",
    "        datapath,\n",
    "        load_data_fn=load_data_func, \n",
    "        val_fraction=0.1,\n",
    "        test_fraction=0.1,\n",
    "    )    \n",
    "    max_val = train_data.get_max_val()\n",
    "    val_config.max_val = max_val\n",
    "    val_data = dataset_class(\n",
    "        val_config,\n",
    "        datapath,\n",
    "        load_data_fn=load_data_func, \n",
    "        val_fraction=0.1,\n",
    "        test_fraction=0.1\n",
    "    )\n",
    "\n",
    "    mean_val, std_val = train_data.compute_mean_std()\n",
    "    train_data.set_mean_std(mean_val, std_val)\n",
    "    val_data.set_mean_std(mean_val, std_val)\n",
    "    data_stats = train_data.get_mean_std()\n",
    "\n",
    "    # NOTE: \"input\" mean & std are computed over the entire dataset and repeated for each channel.\n",
    "    # On the contrary, \"target\" mean & std are computed separately for each channel.\n",
    "    # manipulate data stats to only have one mean and std for the target\n",
    "    assert isinstance(data_stats, tuple)\n",
    "    assert isinstance(data_stats[0], dict)\n",
    "    \n",
    "    data_stats = (\n",
    "        torch.tensor(data_stats[0][\"target\"]),\n",
    "        torch.tensor(data_stats[1][\"target\"])\n",
    "    )\n",
    "\n",
    "    return train_data, val_data, data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_config, val_data_config = get_data_configs()\n",
    "\n",
    "train_dset, val_dset, data_stats = create_dataset(\n",
    "    datapath='/group/jug/federico/careamics_training/data/BioSR',\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    load_data_func=get_train_val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=train_config.batch_size,\n",
    "    num_workers=train_config.num_workers,\n",
    "    shuffle=True\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=train_config.batch_size,\n",
    "    num_workers=train_config.num_workers,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instantiate the lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split_lightning_model(\n",
    "    algorithm: str,\n",
    "    loss_type: str,\n",
    "    img_size: int = 64,\n",
    "    multiscale_count: int = 1,\n",
    "    predict_logvar: Optional[Literal[\"pixelwise\"]] = None,\n",
    "    target_ch: int = 1,\n",
    "    NM_paths: Optional[list[Path]] = None,\n",
    "    training_config: TrainingConfig = TrainingConfig(),\n",
    "    data_mean: Optional[torch.Tensor] = None,\n",
    "    data_std: Optional[torch.Tensor] = None,\n",
    ") -> VAEModule:\n",
    "    \"\"\"Instantiate the muSplit lightining model.\"\"\"\n",
    "    lvae_config = LVAEModel(\n",
    "        architecture=\"LVAE\",\n",
    "        input_shape=img_size,\n",
    "        multiscale_count=multiscale_count,\n",
    "        z_dims=[128, 128, 128, 128],\n",
    "        output_channels=target_ch,\n",
    "        predict_logvar=predict_logvar,\n",
    "        analytical_kl=False, #ok\n",
    "    )\n",
    "\n",
    "    # gaussian likelihood\n",
    "    if loss_type in [\"musplit\", \"denoisplit_musplit\"]:\n",
    "        gaussian_lik_config = GaussianLikelihoodConfig(\n",
    "            predict_logvar=predict_logvar,\n",
    "            logvar_lowerbound=-5.,\n",
    "        )\n",
    "    else:\n",
    "        gaussian_lik_config = None\n",
    "    # noise model likelihood\n",
    "    if loss_type in [\"denoisplit\", \"denoisplit_musplit\"]:\n",
    "        assert NM_paths is not None, \"A path to a pre-trained noise model is required.\"\n",
    "        gmm_list = []\n",
    "        for NM_path in NM_paths:\n",
    "            gmm_list.append(\n",
    "                GaussianMixtureNMConfig(\n",
    "                    model_type=\"GaussianMixtureNoiseModel\",\n",
    "                    path=NM_path,\n",
    "                )\n",
    "            )\n",
    "        noise_model_config = MultiChannelNMConfig(noise_models=gmm_list)\n",
    "        nm = noise_model_factory(noise_model_config)\n",
    "        nm_lik_config = NMLikelihoodConfig(\n",
    "            noise_model=nm,\n",
    "            data_mean=data_mean,\n",
    "            data_std=data_std,\n",
    "        )\n",
    "    else:\n",
    "        noise_model_config = None\n",
    "        nm_lik_config = None\n",
    "\n",
    "    opt_config = OptimizerModel(\n",
    "        name=\"Adamax\",\n",
    "        parameters={\n",
    "            \"lr\": training_config.lr,\n",
    "            \"weight_decay\": 0,\n",
    "        },\n",
    "    )\n",
    "    lr_scheduler_config = LrSchedulerModel(\n",
    "        name=\"ReduceLROnPlateau\",\n",
    "        parameters={\n",
    "            \"mode\": \"min\",\n",
    "            \"factor\": 0.5,\n",
    "            \"patience\": training_config.lr_scheduler_patience,\n",
    "            \"verbose\": True,\n",
    "            \"min_lr\": 1e-12,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    vae_config = VAEAlgorithmConfig(\n",
    "        algorithm_type=\"vae\",\n",
    "        algorithm=algorithm,\n",
    "        loss=loss_type,\n",
    "        model=lvae_config,\n",
    "        gaussian_likelihood_model=gaussian_lik_config,\n",
    "        noise_model=noise_model_config,\n",
    "        noise_model_likelihood_model=nm_lik_config,\n",
    "        optimizer=opt_config,\n",
    "        lr_scheduler=lr_scheduler_config,\n",
    "    )\n",
    "\n",
    "    return VAEModule(algorithm_config=vae_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = \"musplit\" if loss_type == \"musplit\" else \"denoisplit\"\n",
    "lightning_model = create_split_lightning_model(\n",
    "    algorithm=algo,\n",
    "    loss_type=loss_type,\n",
    "    img_size=img_size,\n",
    "    multiscale_count=multiscale_count,\n",
    "    predict_logvar=predict_logvar,\n",
    "    target_ch=target_channels,\n",
    "    NM_paths=nm_paths,\n",
    "    training_config=train_config,\n",
    "    data_mean=data_stats[0],\n",
    "    data_std=data_stats[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set utils for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from careamics.lvae_training.train_utils import get_new_model_version\n",
    "\n",
    "\n",
    "def get_new_model_version(model_dir: Union[Path, str]) -> int:\n",
    "    \"\"\"Create a unique version ID for a new model run.\"\"\"\n",
    "    versions = []\n",
    "    for version_dir in os.listdir(model_dir):\n",
    "        try:\n",
    "            versions.append(int(version_dir))\n",
    "        except:\n",
    "            print(\n",
    "                f\"Invalid subdirectory:{model_dir}/{version_dir}. Only integer versions are allowed\"\n",
    "            )\n",
    "            exit()\n",
    "    if len(versions) == 0:\n",
    "        return \"0\"\n",
    "    return f\"{max(versions) + 1}\"\n",
    "\n",
    "def get_workdir(\n",
    "    root_dir: str,\n",
    "    model_name: str,\n",
    ") -> tuple[Path, Path]:\n",
    "    \"\"\"Get the workdir for the current model.\n",
    "    \n",
    "    It has the following structure: \"root_dir/YYMM/model_name/version\"\n",
    "    \"\"\"\n",
    "    rel_path = datetime.now().strftime(\"%y%m\")\n",
    "    cur_workdir = os.path.join(root_dir, rel_path)\n",
    "    Path(cur_workdir).mkdir(exist_ok=True)\n",
    "\n",
    "    rel_path = os.path.join(rel_path, model_name)\n",
    "    cur_workdir = os.path.join(root_dir, rel_path)\n",
    "    Path(cur_workdir).mkdir(exist_ok=True)\n",
    "\n",
    "    rel_path = os.path.join(rel_path, get_new_model_version(cur_workdir))\n",
    "    cur_workdir = os.path.join(root_dir, rel_path)\n",
    "    try:\n",
    "        Path(cur_workdir).mkdir(exist_ok=False)\n",
    "    except FileExistsError:\n",
    "        print(\n",
    "            f\"Workdir {cur_workdir} already exists.\"\n",
    "        )\n",
    "    return cur_workdir, rel_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"./logs/\"\n",
    "lc_tag = \"with\" if multiscale_count > 1 else \"no\"\n",
    "workdir, exp_tag = get_workdir(ROOT_DIR, f\"{algo}_{lc_tag}_LC\")\n",
    "print(f\"Current workdir: {workdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logger\n",
    "# project_name = \"_\".join((\"careamics\", algo))\n",
    "# if project_name == \"_\".join((\"careamics\", algo)):\n",
    "#     raise ValueError(\"Please create your own project name for wandb.\")\n",
    "custom_logger = WandbLogger(\n",
    "    name=os.path.join(socket.gethostname(), exp_tag),\n",
    "    save_dir=workdir,\n",
    "    project=\"careamics_dataset_tests\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks (e.g., ModelCheckpoint, EarlyStopping, etc.)\n",
    "custom_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=1e-6,\n",
    "        patience=train_config.earlystop_patience,\n",
    "        mode=\"min\",\n",
    "        verbose=True,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        dirpath=workdir,\n",
    "        filename=\"best-{epoch}\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_top_k=1,\n",
    "        save_last=True,\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "    LearningRateMonitor(logging_interval=\"epoch\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configs\n",
    "algo_config = lightning_model.algorithm_config\n",
    "train_data_configs, val_data_config = get_data_configs()\n",
    "\n",
    "# temp -> remove fields that we don't want to save\n",
    "loss_config = deepcopy(asdict(lightning_model.loss_parameters))\n",
    "del loss_config[\"noise_model_likelihood\"]\n",
    "del loss_config[\"gaussian_likelihood\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Configs JSON\n",
    "with open(os.path.join(workdir, \"algorithm_config.json\"), \"w\") as f:\n",
    "    f.write(algo_config.model_dump_json(indent=4))\n",
    "\n",
    "with open(os.path.join(workdir, \"training_config.json\"), \"w\") as f:\n",
    "    f.write(train_config.model_dump_json(indent=4))\n",
    "\n",
    "with open(os.path.join(workdir, \"data_config.json\"), \"w\") as f:\n",
    "    f.write(train_data_configs.model_dump_json(indent=4))\n",
    "    f.write(val_data_config.model_dump_json(indent=4))\n",
    "\n",
    "with open(os.path.join(workdir, \"loss_config.json\"), \"w\") as f:\n",
    "    json.dump(loss_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Configs in WANDB\n",
    "custom_logger.experiment.config.update({\n",
    "    \"algorithm\": algo_config.model_dump()\n",
    "})\n",
    "\n",
    "custom_logger.experiment.config.update({\n",
    "    \"training\": train_config.model_dump()\n",
    "})\n",
    "\n",
    "custom_logger.experiment.config.update({\n",
    "    \"train_data\": train_data_configs.model_dump(),\n",
    "    \"val_data\": val_data_config.model_dump()\n",
    "})\n",
    "\n",
    "custom_logger.experiment.config.update({\n",
    "    \"loss_params\": loss_config\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=train_config.max_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    logger=custom_logger,\n",
    "    callbacks=custom_callbacks,\n",
    "    precision=train_config.precision,\n",
    "    gradient_clip_val=train_config.grad_clip_norm_value, # only works with `accelerator=\"gpu\"`\n",
    "    gradient_clip_algorithm=train_config.gradient_clip_algorithm,\n",
    "    deterministic=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=lightning_model,\n",
    "    train_dataloaders=train_dloader,\n",
    "    val_dataloaders=val_dloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careamics",
   "language": "python",
   "name": "careamics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
