{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation code for the dataset Bla bla\n",
    "\n",
    "Short description of metrics and panels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from careamics.lightning import VAEModule\n",
    "\n",
    "import configs\n",
    "\n",
    "from configs.factory import (\n",
    "    get_algorithm_config,\n",
    "    get_likelihood_config,\n",
    "    get_loss_config,\n",
    "    get_model_config,\n",
    "    get_optimizer_config,\n",
    "    get_training_config,\n",
    "    get_lr_scheduler_config,\n",
    ")\n",
    "from datasets import create_train_val_datasets\n",
    "from utils.callbacks import get_callbacks\n",
    "from utils.io import get_workdir, log_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.parameters import get_denoisplit_parameters\n",
    "from configs.data import get_data_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO refactor, all functions should come from careamics\n",
    "train_data_config, val_data_config, test_data_configs = get_data_configs()\n",
    "params = get_denoisplit_parameters()\n",
    "loss_config = get_loss_config(**params)\n",
    "model_config = get_model_config(**params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **params\n",
    ")\n",
    "training_config = get_training_config(**params)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**params)\n",
    "optimizer_config = get_optimizer_config(**params)\n",
    "\n",
    "algo_config = get_algorithm_config(\n",
    "    algorithm=params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add mode train/test to return different datasets\n",
    "train_dset, val_dset, test_dset, data_stats = create_train_val_datasets(\n",
    "    datapath=data_path,\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=load_data_fn,\n",
    ")\n",
    "train_dloader = DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    num_workers=params[\"num_workers\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    num_workers=params[\"num_workers\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model and load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = create_split_lightning_model(\n",
    "        algorithm=\"denoisplit\",\n",
    "        loss=\"denoisplit_musplit\",\n",
    "        model_parameters={\"img_size\": img_size,\n",
    "        \"multiscale_count\": multiscale_count,\n",
    "        \"predict_logvar\": predict_logvar,\n",
    "        \"target_ch\": target_channels,\n",
    "        \"nm_paths\": nm_paths},\n",
    "        data_config={\"data_stats\": data_stats},\n",
    "        training_config=training_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(ckpt_dir):\n",
    "    ckpt_fpath = get_model_checkpoint(ckpt_dir, mode=which_ckpt)\n",
    "else:\n",
    "    assert os.path.isfile(ckpt_dir)\n",
    "    ckpt_fpath = ckpt_dir\n",
    "\n",
    "print(f\"Loading checkpoint from: '{ckpt_fpath}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(ckpt_fpath)\n",
    "\n",
    "lightning_model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "lightning_model.eval()\n",
    "lightning_model.cuda()\n",
    "\n",
    "print('Loading weights from epoch', checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: here, patch-wise PSNR is used, hence results are not trustworthy\n",
    "# TODO rename, put stitching inside\n",
    "pred_tiled = get_dset_predictions(\n",
    "  model=lightning_model,\n",
    "  dset=test_dset,\n",
    "  batch_size=batch_size,\n",
    "  num_workers=num_workers,\n",
    "  mmse_count=mmse_count,\n",
    "  loss_type=algo_config[\"loss\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch the std of the predictions (i.e., std computed on the mmse_count predictions)\n",
    "if pred_tiled.shape[-1] != test_dset.get_img_sz():\n",
    "    pad = (val_dset.get_img_sz() - pred_tiled.shape[-1] )//2\n",
    "    pred_tiled = np.pad(pred_tiled, ((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "\n",
    "# Stitch tiled predictions\n",
    "pred = stitch_predictions_new(\n",
    "    pred_tiled,\n",
    "    test_dset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO discuss visualing inidividual samples vs mmse. do both! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel 1 ..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
