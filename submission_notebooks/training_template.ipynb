{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for Dataset BlaBla\n",
    "\n",
    "This is a notebook that shows how to train a model for the dataset BlaBla.\n",
    "\n",
    "# What it actually does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from careamics.lightning import VAEModule\n",
    "\n",
    "from configs.factory import (\n",
    "    get_algorithm_config,\n",
    "    get_likelihood_config,\n",
    "    get_loss_config,\n",
    "    get_model_config,\n",
    "    get_optimizer_config,\n",
    "    get_training_config,\n",
    "    get_lr_scheduler_config,\n",
    ")\n",
    "from datasets import create_train_val_datasets\n",
    "from utils.callbacks import get_callbacks\n",
    "from utils.io import get_workdir, log_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_train_val_data_nikola, create_train_val_datasets # TODO rename ?\n",
    "from configs.nikolaData import get_data_configs\n",
    "from configs.parameters import get_denoisplit_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_config, val_data_config, test_data_configs = get_data_configs()\n",
    "\n",
    "train_dset, val_dset, _, data_stats = create_train_val_datasets(\n",
    "    datapath=data_path,\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,  # TODO: check this\n",
    "    load_data_func=load_data_fn,\n",
    ")\n",
    "    train_dloader = DataLoader(\n",
    "        train_dset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_dloader = DataLoader(\n",
    "        val_dset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "params = get_denoisplit_parameters()\n",
    "loss_config = get_loss_config(**params)\n",
    "model_config = get_model_config(**params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **params\n",
    ")\n",
    "training_config = get_training_config(**params)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**params)\n",
    "optimizer_config = get_optimizer_config(**params)\n",
    "# TODO: all the previous configs can also be istantiated from a single function...\n",
    "algo_config = get_algorithm_config(\n",
    "    algorithm=params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")\n",
    "\n",
    "# log configs\n",
    "dirname = f\"{params['algorithm']}_{str(train_data_config.data_type).split('.')[-1]}\"\n",
    "logdir, _ = get_workdir(root_path, dirname)\n",
    "print(f\"Log directory: {logdir}\")\n",
    "train_data_config.data_path = data_path\n",
    "custom_logger = log_configs(\n",
    "    configs=[algo_config, training_config, train_data_config, loss_config],\n",
    "    names=[\"algorithm\", \"training\", \"data\", \"loss\"],\n",
    "    log_dir=logdir,\n",
    "    wandb_project=wandb_project,\n",
    ")\n",
    "# init lightning model\n",
    "lightning_model = VAEModule(algorithm_config=algo_config)\n",
    "\n",
    "# train the model\n",
    "custom_callbacks = get_callbacks(logdir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=training_config.num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    logger=custom_logger,\n",
    "    callbacks=custom_callbacks,\n",
    "    precision=training_config.precision,\n",
    "    gradient_clip_val=training_config.gradient_clip_val,\n",
    "    gradient_clip_algorithm=training_config.gradient_clip_algorithm,\n",
    ")\n",
    "trainer.fit(\n",
    "    model=lightning_model,\n",
    "        train_dataloaders=train_dloader,\n",
    "        val_dataloaders=val_dloader,\n",
    "    )\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset, val_dset, _, data_stats = create_train_val_datasets(\n",
    "    datapath=data_path,\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,  # TODO: check this\n",
    "    load_data_func=load_data_fn,\n",
    ")\n",
    "    train_dloader = DataLoader(\n",
    "        train_dset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_dloader = DataLoader(\n",
    "        val_dset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
